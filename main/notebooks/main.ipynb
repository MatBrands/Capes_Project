{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_parquet('../datasets/processed/frequency_matrix_2013.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    new_text = [item for item in wordpunct_tokenize(text) if item not in stop_words]\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_stem(text: str) -> str:\n",
    "    stemmer = LancasterStemmer()\n",
    "    words = [stemmer.stem(word) for word in wordpunct_tokenize(text)]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = 'Este trabalho foi desenvolvido utilizando as tecnologias Python, Pandas, Numpy, NLTK, Unidecode e LancasterStemmer. O objetivo é realizar a limpeza de um texto, removendo stopwords, pontuações e acentuações, além de realizar a redução de palavras para sua forma raiz. O texto utilizado para teste foi o resumo deste trabalho. nitrosilo complexo rutenio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = resumo.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = re.sub(r'[^a-zA-Z0-9\\s]+', '', resumo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumo = remove_stopwords(resumo)\n",
    "resumo = lc_stem(resumo)\n",
    "resumo = unidecode(resumo)\n",
    "\n",
    "del remove_stopwords, lc_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trabalho desenvolvido utilizando tecnologia python panda numpy nltk unidecod lancasterstem objetivo realiz limpez texto removendo stopword pontua acentua alm realiz reduo palavra form raiz texto utilizado test resumo dest trabalho nitrosilo complexo rutenio'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_concat(i: int, item: str, text: str) -> str:\n",
    "    elements = wordpunct_tokenize(text)[i:]\n",
    "    \n",
    "    if len(elements) > 1:\n",
    "        test_1 = item + ' ' + elements[1]\n",
    "        test_2 = item  + elements[1]\n",
    "    else:\n",
    "        return i, item\n",
    "    \n",
    "    if test_1 in matrix.palavra_chave.values:\n",
    "        return verify_concat(i+1, test_1, text)\n",
    "    if test_2 in matrix.palavra_chave.values:\n",
    "        return verify_concat(i+1, test_2, text)\n",
    "    else:\n",
    "        return i, item\n",
    "\n",
    "def get_keywords(text: str) -> list:\n",
    "    result = []\n",
    "    i = 0\n",
    "    elements = wordpunct_tokenize(text)\n",
    "    while i < len(elements):\n",
    "        item = elements[i]\n",
    "        if item in matrix.palavra_chave.values:\n",
    "            i, item = verify_concat(i, item, text)\n",
    "            result.append(item)\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texto',\n",
       " 'utilizando',\n",
       " 'nltk',\n",
       " 'test',\n",
       " 'nitrosilo complexo rutenio',\n",
       " 'limpez',\n",
       " 'palavra',\n",
       " 'raiz',\n",
       " 'python',\n",
       " 'objetivo',\n",
       " 'alm',\n",
       " 'tecnologia',\n",
       " 'resumo',\n",
       " 'form',\n",
       " 'trabalho']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = list(set(get_keywords(resumo)))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20023"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix[matrix.palavra_chave.str.fullmatch(result[0])].sort_values(by='frequencia', ascending=False)\n",
    "matrix[matrix.palavra_chave.str.fullmatch(result[0])].frequencia.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto: 0.04344467049019401\n",
      "utilizando: 0.02659665239318774\n",
      "nltk: 4.339476650870899e-06\n",
      "test: 0.06401812929197295\n",
      "nitrosilo complexo rutenio: 3.6885551532402644e-05\n",
      "limpez: 0.0010544928261616284\n",
      "palavra: 0.006162056844236677\n",
      "raiz: 0.002759907149953892\n",
      "python: 3.4715813206967194e-05\n",
      "objetivo: 0.09714786378304682\n",
      "alm: 0.055256725933864596\n",
      "tecnologia: 0.007871810644679811\n",
      "resumo: 0.0027946229631608593\n",
      "form: 0.2398927784751196\n",
      "trabalho: 0.12835737985611034\n"
     ]
    }
   ],
   "source": [
    "frequency_general = matrix.frequencia.sum()\n",
    "\n",
    "for item in result:\n",
    "    this_frequency = matrix[matrix.palavra_chave.str.fullmatch(item)].frequencia.sum()\n",
    "    print(f'{item}: {(this_frequency/frequency_general) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto: linguística e literatura - 0.22056152369090248\n",
      "utilizando: química - 0.07176777644514405\n",
      "nltk: linguística e literatura - 0.00010591189613008524\n",
      "test: educação física - 0.22978591219598782\n",
      "nitrosilo complexo rutenio: ciências ambientais - 0.0005564107795315021\n",
      "limpez: geografia - 0.007642830421683113\n",
      "palavra: linguística e literatura - 0.02022917216084628\n",
      "raiz: ciências agrárias i - 0.019929185401145796\n",
      "python: ciência da computação - 0.0004810994083680026\n",
      "objetivo: educação física - 0.13770513627677483\n",
      "alm: ciências biológicas ii - 0.09491128115669825\n",
      "tecnologia: ensino - 0.035947116656096505\n",
      "resumo: zootecnia / recursos pesqueiros - 0.013288488041870817\n",
      "form: comunicação e informação - 0.538162865537988\n",
      "trabalho: matemática / probabilidade e estatística - 0.34520993163137503\n"
     ]
    }
   ],
   "source": [
    "subarea_groupped = matrix.groupby('subarea').frequencia.sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "for item in result:\n",
    "    words_frequency = matrix[matrix.palavra_chave.str.fullmatch(item)].sort_values(by='frequencia', ascending=False).loc[:, ['subarea', 'frequencia']].copy()\n",
    "    matrix_corr = words_frequency.join(subarea_groupped.set_index('subarea'), on='subarea', how='left', lsuffix='_this_word', rsuffix='_all_groupped')\n",
    "    matrix_corr['corr'] = (matrix_corr.frequencia_this_word / matrix_corr.frequencia_all_groupped) * 100\n",
    "    corr = matrix_corr.sort_values(by='corr', ascending=False).reset_index(drop=True).loc[0, ['subarea', 'corr']]\n",
    "    print(f'{item}: {corr[0]} - {corr[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster_capes_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
